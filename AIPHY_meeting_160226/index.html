<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>AIPHY Midterm 02 2026</title>

    <meta name="description" content="AIPHY_midterm_02_2026">
    <link rel="stylesheet" href="../assets/reveal/dist/reveal.css">
    <link rel="stylesheet" href="../assets/reveal/dist/theme/darkenergy.css" id="theme">

    <!-- Highlight.js library -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/monokai.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>

    <!-- Include Bokeh CSS and JS -->
    <link rel="stylesheet" href="https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.css" type="text/css">
    <script src="https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js"></script>

    <!-- Include math -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- code rendering-->
    <link rel="stylesheet" href="plugin/highlight/monokai.css" />
    <script src="plugin/highlight/highlight.js"></script>

    <!-- Box creations -->
    <style>
        /* Basic styling for the vertical boxes */
        .double_box_container {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            height: 100%;
            padding: 20px;
        }

        .double_box {
            background: rgba(0, 0, 0, 0.7);
            /* Dark background with transparency */
            color: lightgray;
            border-radius: 8px;
            padding: 20px;
            width: 44%;
            display: flex;
            flex-direction: column;
        }

        .box-title {
            background: rgba(255, 255, 255, 0.2);
            /* Lighter background for title */
            color: white;
            font-weight: bold;
            padding: 10px;
            border-radius: 4px;
            margin-bottom: 10px;
        }

        /* Container to center and stack boxes vertically */
        .vertical-center-container {
            display: flex;
            flex-direction: column;
            /* Stack children vertically */
            align-items: center;
            /* Center children horizontally */
            gap: 20px;
            /* Space between boxes */
            height: 100vh;
            /* Full viewport height (optional) */
            padding: 20px;
        }

        /* Styling for the boxes */
        .centered-box {
            background: rgba(0, 0, 0, 0.7);
            /* Dark background with transparency */
            color: lightgray;
            border-radius: 8px;
            padding: 20px;
            width: 80%;
            /* Width of the boxes */
            max-width: 800px;
            /* Max width to limit size */
        }

        .reveal .slide-title {
            border-left: 5px solid white;
            text-align: left;
            margin-left: 20px;
            padding-left: 20px;
        }

        .bokeh-iframe {
            width: 80vw;
            height: 80vh;
            border: none;
        }
        .reveal section[data-background-image="img_contents/LHC.jpg"] h1,
        .reveal section[data-background-image="img_contents/LHC.jpg"] h2,
        .reveal section[data-background-image="img_contents/LHC.jpg"] h3,
        .reveal section[data-background-image="img_contents/LHC.jpg"] h4,
        .reveal section[data-background-image="img_contents/LHC.jpg"] h5,
        .reveal section[data-background-image="img_contents/LHC.jpg"] h6,
        .reveal section[data-background-image="img_contents/LHC.jpg"] p,
        .reveal section[data-background-image="img_contents/LHC.jpg"] span,
        .reveal section[data-background-image="img_contents/LHC.jpg"] li {
            background: rgba(0, 0, 0, 0.45);
            padding: 0.1em 0.25em;
            border-radius: 4px;
            display: inline-block;
            backdrop-filter: blur(2px);

        }
        .reveal mjx-container[display="true"] {
            display: inline-block !important;
            width: auto !important;
            max-width: none !important;
            background: rgba(0,0,0,0.25);
            color: #fff;
            padding: 0.4em 0.65em;
            border-radius: 12px;
            border: 1.5px solid rgba(255,255,255,0.45);
        }
        .reveal section[data-background-image="img_contents/LHC.jpg"]
        mjx-container[display="false"] {
            background: none;
            padding: 0;
        }
        .title-panel {
        background: rgba(0, 0, 0, 0.45);
        padding: 1.7rem 2.6rem;   
        border-radius: 12px;
        max-width: 68%;          
        margin: auto;
        backdrop-filter: blur(2px);
        }

        .title-panel h2 {
        color: #ffffff;
        font-size: 2.1em;        
        margin-bottom: 0.45em;
        }

        .title-panel h3 {
        color: #e0e0e0;
        font-weight: 300;
        font-size: 1em;       
        line-height: 1.3;
        margin-bottom: 1em;
        }

        .title-panel .meta {
        color: #cccccc;
        font-size: 0.75em;       
        line-height: 1.4;
        }

        .title-panel .affiliation {
        opacity: 0.75;
        }
    </style>
</head>

<body>
    <div class="reveal">
        <div class="slides">
            <!-- Slide 1 -->
            <section data-background-image="img_contents/gen_intro_img.png">
                <div class="title-panel">
                    <h2>Interpretable Deep Learning for Fundamental Physics</h2>
                    <h3>
                    From Supernovae detection to Neutrinos passing by Parton Distribution Functions.
                    </h3>

                    <p class="meta">
                    <strong>Raphaël Bonnet-Guerrini<sup>1</sup></strong><br>
                    16/02/2026<br>
                    <span class="affiliation">
                        <sup>1</sup>Computer Science Department, University of Milan
                    </span>
                    </p>
                </div>
            </section>



            <!-- PART A: ML4TRANSIENTS-->
            <!-- Slide 2 ML4transients title -->
            <section data-background-image="img_contents/vera_rubin_obs.png">
                <h2>Interpretable Deep Learning for Real/Bogus classification with Uncertainty Quantification</h2>
                <span style="font-size: smaller;"><b>Raphaël Bonnet-Guerrini</b><sup>1</sup>, Dominique Fouchez<sup>2</sup>, Vincenzo Piuri<sup>1</sup>, Benjamin Racine<sup>2</sup>, and Bruno Sanchez<sup>2</sup></span>
                <br>
                <br>
                <span style="font-size: small; text-align:center"><sup>1</sup>Computer Science Department, University of Milan;       <sup>2</sup>Centre de Physique des Particules de Marseille</span>

            </section>

            <!-- Slide 3 SN and LSST presentation title -->
             <section>
                <section data-auto-animate data-background-image="img_contents/vera_rubin_obs.png">
                    <h3 class="slide-title">Cosmology: a data driven era.</h3>
                    <div style="text-align:left">Vera Rubin Observatory has started operations in 2025. 10 years survey. Expectations are up to 100k spectroscopically confirmed Type Ia supernovae, up to 1M in total. 15PB of data.
                    </div>  
                    <div class="fragment"style="text-align:left"> 
                    1 image every every 60 seconds during 10 years. 
                    </div>
                    <div class="fragment" style="text-align:left">                  
                    <ul>
                    <li>Calibrate the image</li>
                    <li>Find all differences (~10k/image)</li>
                    <li>
                        For each difference
                        <ul>
                        <li>Perform photometry</li>
                        <li>Crossmatch with known catalogs</li>
                        <li>Find nearest Solar System objects</li>
                        <li>Compute various features</li>
                        <li>Find past LSST detections at this location</li>
                        <li>Package into an alert</li>
                        </ul>
                    </li>
                    </ul>

                </section>
            </section>

            <!-- Slide 4 DIA -->
            <section data-auto-animate data-background-image="img_contents/vera_rubin_obs_2.png">
                <!-- Slide 4.1 -->
                <section >
                    <h3 class="slide-title">Difference Image analysis: </h3>
                    <div class=" container" style="align-items: start; text-align: center;">
                        <div class="col">
                            <br>
                            <br>                                
                            <img src="img_contents/DIA.png" style="width: 300px; height: auto;">
                        </div>
                        <div class="fragment col">
                            <img src="img_contents/key_intuition3.png" style="width: 700px; height: auto;">
                        </div>
                    </div>
                </section>

                <!-- Slide 4.2 -->      
                <section data-auto-animate data-background-image="img_contents/vera_rubin_obs_2.png">
                    <h3 class="slide-title">Weakly supervised learning problem</h3>
                    <div style="text-align: left;">
                        No ground truth. During training, we are interested in the <b>False Positives</b>
                    </div>
                    <div class="container" style="align-items: start; text-align: center;">
                        <div class="col">
                            <br><br><br><br><br><br><br><br>
                            <img src="img_contents/key_intuition3.png" style="width: 700px; height: auto;">

                        </div>
                        <div class="col" style="align-items: start; text-align: right;"">
                            <img class ="fragment" src="img_contents/key_intuition4.png" style="width: 700px; height: auto;">
                        </div>

                    
                    </div>
                </section>
            </section>

            <!-- Slide 5 : Data Presentation -->
            <section>      
                <!-- Slide 5.1 Training data -->
                <section data-background-image="img_contents/vera_rubin_obs_2.png">
                    <h3 class="slide-title">Training Data Presentation</h3>
                    The HSC RC2 subset is composed of 6 detectors, with 8 visits per filter.
                    <div class="container" style="align-items: start; text-align: left;">
                        <div class="col" style="margin-top: 10px;">
                            <br><br>
                            <h4>Producing the Cutouts: </h4>
                            <li>Cutout coordinates are extracted from the DIA source tables and produced from the Calexps.</li>
                            <li>Final format: (30x30), normalized grayscale.</li>
                            <br><br>
                            <span>
                                <h4>Classes and Labels: </h4>
                                <li>Inferring the injected data from the matchDiaSrc.</li>
                            </span>
                        </div>
                        <div class="col" style="margin-top: 10px;">
                            <br>
                            <img src="img_contents/cutouts_examples.png" style="width: 500px; height: auto;">
                        </div>
                    </div>
                </section>
                <!-- Slide 5.2 Evaluation data -->
                <section data-background-image="img_contents/vera_rubin_obs_2.png">
                    <h3 class="slide-title">Evaluation Data Presentation</h3>
                    <div class="container" style="align-items: start; text-align: left;">
                    We use the full UDEEP dataset to build light curves. It is composed of 862 visits across 103 detectors. Those light curves are first filtered and then manually inspected for transient identification.
                    </div>
                    <div class="container" style="align-items: start; text-align: left;">
                        <div class="col" style="margin-top: 10px;">
                            <br>
                            <br>
                            <br>
                            <h4>Filtering steps: from 8M to 342 LCs </h4>

                            <h4>Human labeling: </h4>
                                <img src="img_contents/table_labels.png" style="width: 500px; height: auto;">
                            
                        </div>
                        <div class="col" style="margin-top: 10px;">
                            <br>
                            <img src="img_contents/band_dia.png" style="width: 200px; height: auto;">
                            <img src="img_contents/lc_sn.png" style="width: 500px; height: auto;">
                        </div>
                    </div>
                </section>
            </section>

            <!-- Slide 6 : Network presentation -->
            <section>
                <section data-background-image="img_contents/vera_rubin_obs_2.png">
                    <h3 class="slide-title">Network Architecture</h3>
                    <div style='text-align: left'>
                        A simple CNN architecture:
                        <li>3 Conv blocks consists of convolution, batch normalization, ReLU, max pooling, and dropout.</li>
                        <li>2 fully connected layers (FCC) for high-level abstractions and classification output.</li>
                        <li>Dropout layers for generalization.</li>
                    </div>
                    <img src="img_contents/NN_visu.png" style="width: 700px; height: auto;">
                </section>
            </section>
            <!-- Slide 7 : WSL -->
            <section>
                <!-- Slide 7.1 -->
               <section data-background-image="img_contents/vera_rubin_obs_2.png" style="text-align:left">
                    <h3 class="slide-title">Co-Teaching: A Self-Event-Selection Strategy for Weakly Supervised Learning</h3>
                    <p>Two models are trained simultaneously with different views on the same dataset.</p>
                    <div class="container" style="align-items: start; text-align: left">
                        <div class="col" style="display: flex; flex-direction: column; align-items: center;">
                            <br><br>
                            <img data-src="img_contents/coteaching.svg" class="plain" style="width: 300px;">
                                <div style="text-align: center;">
                                <br>
                                <a href="https://arxiv.org/abs/1804.06872">
                                    <img src="https://img.shields.io/badge/cs-arXiv.1804.06872-B31B1B.svg"
                                        class="plain" style="height:25px;" />
                                </a>
                            </div>
                        </div>
                        <div class="col">
                            <p>In each batch, each model selects the datum with the smallest loss (most confident predictions).</p>
                            <p>Avoid training on the wrong labels.</p>
                            <strong>Pros:</strong>
                            <ul>
                                <li>Effective for noisy datasets.</li>
                            </ul>
                            <strong>Cons:</strong>
                            <ul>
                                <li>Increases computational cost.</li>
                                <li>Assumes symmetrical noise.</li>
                            </ul>
                        </div>
                    </div>
                </section>
            <!-- Slide 7.2 -->
               <section data-background-image="img_contents/vera_rubin_obs_2.png" style="text-align:left">
                    <h3 class="slide-title">Asymmetrical Co-Teaching: </h3>
                    <p>Two models are trained simultaneously with different views on the same dataset.</p>
                    <div class="container" style="align-items: start; text-align: left">
                        <div class="col" style="display: flex; flex-direction: column; align-items: center;">
                            <br><br>
                            <img data-src="img_contents/coteaching_comparison.svg" class="plain" style="width: 300px;">
                        </div>
                        <div class="col">
                            <br>
                            <h4>Key Changes:</h4>
                            <ul>
                                <li>Implements different remembering rates for each class.</li>
                                <li>Better fits the needs of our asymmetrically weakly supervised dataset.</li>
                            </ul>
                        </div>
                    </div>
                </section>
            </section>

            <!-- Slide 8 : Result WSL -->
            <section data-background-image="img_contents/vera_rubin_obs_2.png" style="text-align:left">
                <h3 class="slide-title">WSL analysis:</h3>
                <div class="container" style="align-items: start; text-align: left">
                    <div class="col">
                        Four different training sets with varying noise levels, to compare the performances. 
                        <br> 
                        <ul>
                            <li>Co-Teaching methods becomes better for higher noise levels.</li>
                            <li>Asym-Co-Teaching shows improved performance.</li>
                            <li>Demonstrate the effectiveness of the approach in real-world scenarios and in future better calibrated photometry context.</li>

                        </ul>
                    </div>
                    <div class="col">
                        <img src="img_contents/WSL_results.png" style="width: 700px; height: auto;">
                    </div>
                </div>
             </section>


            <!-- Slide 9 : Uncertainty Quantification -->
            <section data-background-image="img_contents/vera_rubin_obs_2.png">
                <!-- Slide 9.1 Ensembles and MC Dropout -->
                <section>
                    <h3 class="slide-title">Ensembles and MC Dropout</h3>
                    <div class="container" style="align-items: start; text-align: left">
                        <div class="col">
                            <h4>Ensembles:</h4>
                            <ul>
                                <li>Train multiple models with different initializations.</li>
                                <li>Average predictions to reduce variance and improve robustness.</li>
                                <li>Cost: T trainings and T inferences.</li>
                            </ul>
                            <div style="text-align:center">
                                <a href="https://arxiv.org/abs/1612.01474" style="text-align:center">
                                    <img src="https://img.shields.io/badge/stat.ML-arXiv%3A1612.01474-B31B1B?logo=arxiv" height="25">
                                </a>
                            </div>
                        </div>
                        <div class="col">
                            <h4>MC Dropout:</h4>
                            <ul>
                                <li>Apply dropout during inference to simulate an ensemble of models.</li>
                                <li>Perform multiple forward passes to estimate uncertainty.</li>
                                <li>Cost: T inferences.</li>
                            </ul>
                            <div style="text-align:center">
                                <a href="https://arxiv.org/abs/1506.02142" style="text-align:center">
                                    <img src="https://img.shields.io/badge/stat.ML-arXiv%3A1506.02142-B31B1B?logo=arxiv" height="25">
                                </a>
                            </div>
                        </div>
                    </div>
                    <div>
                        <p>
                            <strong>Predictive uncertainty (from variance of predictions):</strong>
                        </p>

                        <p>
                            \[
                            \hat{\mu}(x)=\frac{1}{T}\sum_{t=1}^{T}p_t(x),
                            \qquad
                            \widehat{\mathrm{Var}}(x)=\frac{1}{T-1}\sum_{t=1}^{T}\left(p_t(x)-\hat{\mu}(x)\right)^{2}
                            \]
                        </p>

                        <p style="text-align: left;">
                            where \( p_t(x)=\begin{cases} \sigma(f_t(\mathbf{x}, \hat\theta_t)), & \text{for ensemble}\\ \sigma(f_t(\mathbf{x}, \hat\theta, z_t)), & \text{for MC Dropout} \end{cases} \)
                        </p>
                    </div>
                </section>

                <!-- Slide 9.2 UQ for Co-Teaching -->
                <section>
                    <h3 class="slide-title">UQ for Co-Teaching</h3>
                        <div style="text-align: left;">
                            Co-Teaching methods trains two models simultaneously. <br>
                            Despite not being completely independent, we can still use them to estimate uncertainty by treating their predictions as an ensemble. <br>
                            \( N=2\) ensemble is small, we can extend it by performing M stochastic forward passes with MC Dropout for each model, resulting in a total of \(NM\) predictions.
                            

                            \[    
                            \bar{p}(\mathbf{x}^*) = \frac{1}{NM} \sum_{n=1}^{N} \sum_{m=1}^{M} p_{n,m}(\mathbf{x}^*),
                            \]
                            and 
                            \[
                            \mathrm{Var}_{\text{Co-Ens-Dropout}}(\mathbf{x}^*) = \frac{1}{NM} \sum_{n=1}^{N}\sum_{m=1}^{M}\big(p_{n,m}(\mathbf{x}^*) - \bar{p}(\mathbf{x}^*)\big)^2.
                            \]
                        </div>
                </section>
                
                <!-- Slide 9.3 Metrics -->
                <section>
                    <h3 class="slide-title">Metrics</h3>
                    <div class="container" style="align-items: start; text-align: left">
                        <div class="col">
                            <br><br>
                            <h4>Calibration Metrics:</h4>
                            <ul>
                                <li>Negative Log-Likelihood (NLL): quality of probabilistic predictions.</li>
                                <li>Brier Score: MSE of probabilistic predictions.</li>
                                <li>Expected Calibration Error (ECE): difference between predicted probabilities and observed frequencies.</li>
                            </ul>
                        </div>
                        <div class="col">
                            <br><br>
                            <h4>Correlations with Physical Quantities:</h4>
                            <ul>
                                <li>Correlations with Signal-to-Noise Ratio (SNR).</li>
                                <li>Correlations with maximum brightness of a SNIa.</li>
                            </ul>
                            <span>Correlations are computed using Spearman's rank correlation coefficient, which measures the strength and direction of the monotonic relationship between two variables.</span><br>
                            \[
                                \rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}  
                            \]
                        </div>
                    </div>
                    
                </section>
    
            </section>
            
            <!-- Slide 10: Results of the UQ -->
            <section data-background-image="img_contents/vera_rubin_obs_2.png" style="text-align:left">
                <h3 class="slide-title">UQ analysis:</h3>
                <div class="container" style="align-items: start; text-align: left">
                    <div class="col">
                        <img src="img_contents/calib_metric.png" style="width: 700px; height: auto;">
                        <img src="img_contents/physical_metric.png" style="width: 700px; height: auto;">
                    </div>
                    <div class="col">
                        <img src="img_contents/UQ_calib.png" style="width: 700px; height: auto;">
                    </div>
                </div>
                <div>
                    <li>All methods shows correlation in the correct direction with physical values.</li> 
                    <li>Our methods surprisingly beats all other methods while costing \(25\) times less training power than ensemble based.</li>
                </div>
             </section>
            
            <!-- Slide 11: Interpretability -->
            <section data-background-image="img_contents/vera_rubin_obs_2.png">
                <!-- Slide 11.1 -->
                <section>
                        <h3 class="slide-title">Using UMAP for NN Latent Space visualization.</h3>
                        <div>
                            <div style="text-align:center">
                                <a href="https://arxiv.org/abs/1802.03426" style="text-align:center">
                                    <img src="https://img.shields.io/badge/stat.ML-arXiv%3A1802.03426-B31B1B?logo=arxiv" height="25">
                                </a>
                            </div>
                            <h4>UMAP Overview:</h4>
                            <ul>
                                <li>Preserves both local and global structure using non-linear dimensionality reduction.</li>
                                <li>Builds a nearest-neighbors graph and optimizes it for lower dimensions.</li>
                            </ul>
                        </div>
                        <div style="text-align:center">
                            <img src="img_contents/umap_explanation.png" style="width: 600px; height: auto;"></img>
                        </div>
                        <div class="fragment">
                            <h4>UMAP in Our Case:</h4>
                            <ul>
                                <li>Applied to the output layer of the network.</li>
                                <li>Provides a visual tool for better understanding network classifications.</li>
                            </ul>
                        </div>
                </section>
                <!-- Slide 11.2 -->
                <section>
                    <h3 class="slide-title">UMAP Results</h3>
                    <div style="text-align:center">
                        <iframe src="plot_contents/umap_classes.html" class="bokeh-iframe"></iframe>
                    </div>
                </section>
                <section>
                    <h3 class="slide-title">UMAP Results: SNR overlay</h3>
                    <div style="text-align:center">
                        <iframe src="plot_contents/umap_SNR.html" class="bokeh-iframe"></iframe>
                    </div>
                </section>
                <section>
                    <h3 class="slide-title">UMAP Results: UQ overlay</h3>
                    <div style="text-align:center">
                        <iframe src="plot_contents/umap_UQ.html" class="bokeh-iframe"></iframe>
                    </div>
                </section>
             </section>

            <!-- Slide 12: Outcomes --> 
            <section data-auto-animate data-background-image="img_contents/vera_rubin_obs.png">
                <h3 class="slide-title">Outcomes of the project:</h3>
                <div class="fragment" style="text-align: left;">
                Conclusions: 
                <ul>
                    <li>New Asym-Co-Teaching methods that allows a mitigation of the risk in a high stake class.</li>
                    <li>Novel uncertainty quantification method for Co-Teaching, providing better calibrated uncertainties at lower costs.</li>
                    <li>UMAP visualization of the latent space confirms our interpretation on global model behavior.</li>
                </ul>
                </div>
                <div class="fragment" style="text-align: left;">
                Future work:
                <ul>
                    <li>Further exploration of the latent space to identify specific features or patterns associated with bogus class.</li>
                    <li>Expanding work to upcoming datasets.</li>
                    <li>Exploration of the MC-Ensemble mixture systematic performances.</li>
                </ul>
                </div>

            </section>























            <!-- PART B: SHAPLEY VALUES-->

            <!-- Slide 13: intro NNPDF -->
            <section data-background-image="img_contents/LHC.jpg">
                <h2 >Shapley Values meet NNPDF</h2>
                <span  style="font-size: smaller;"><b>Raphaël Bonnet-Guerrini</b><sup>1</sup>, Stefano Carrazza<sup>2</sup>, Dakshansh Chawda <sup>2</sup>, Stefano Forte<sup>2</sup>, Eva Groenendijk<sup>2</sup>, Vincenzo Piuri<sup>1</sup>, and Ramon Winterhalder<sup>2</sup></span>
                <br>
                <br>
                <span  style="font-size: small; text-align:center"><sup>1</sup>Computer Science Department, University of Milan;         <sup>2</sup>Physics Department, University of Milan</span>
            </section>

            <!-- Slide 14: Context -->
            <section data-background-gradient="linear-gradient(to bottom, black, grey)">
                <!-- Slide 14.1 -->
                <section>
                    <h3 class="slide-title highlight">Parton Distribution Functions (PDFs) and NNPDF</h3>
                    <div  style="text-align: left;">
                        <div class="container">
                            <div class="col text-cente;">
                                                        <img src="img_contents/pdf3.1.png" style="width: 300px; height: auto; display: block; margin: 0 auto;">
                            <div style="text-align: center;">
                                \[
                                f(x, Q_0^2) = x^a(1-x)\text{NN}(\phi(x))
                                \]
                            </div>
                            </div>
                            <div class="col">
                                <ul>
                                    <li>Describe the probability of finding a parton carrying a fraction \(x\) of the proton's momentum at a given energy scale \(Q^2\).</li>
                                    <li>Essential for predicting outcomes of high-energy particle collisions.</li>
                                    <li>NNPDF uses neural networks to model PDFs without assuming a specific functional form.</li>
                                    <li>Trained on experimental data from different detectors.</li>

                                </ul>
                                <div style="text-align:center">
                                    <a href="https://arxiv.org/abs/2109.02653" style="text-align:center">
                                        <img src="https://img.shields.io/badge/hep--ph-arXiv%3A2109.02653-B31B1B?logo=arxiv" height="25">
                                    </a>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </section>
                <!-- Slide 14.2 -->
                <section>
                <h3 class="slide-title highlight" style="text-align: left;">Shapley Values For NNPDF</h3>
                    <p style="text-align: left;">    
                        NNPDF solves an inverse problem: we have a set of experimental data and we want to find the underlying PDFs that best explain the data.
                    </p>
                    <img data-src="img_contents/NNPDF.png" style="width: 700px; height: auto;" class="plain"></img>
                    <p style="text-align: left;">    
                        Theory (QCD, QED, EW) allows to compute the Observables (\(\mathcal{O_n}\)) values from the PDF space, and the inverse done fitting precise experimental data from LHC . 
                        
                    </p>
                </section>

                <!-- Slide 14.3 -->
                <section>
                    <h3 class="slide-title highlight" style="text-align: left;">PDF space</h3>
                    <p style="text-align: left;">    
                        The PDF space is a high-dimensional space, (9 in the flavor basis and 14 in the evolution basis). Question is why a given flavor looks like this in a given x region? What are the dataset responsible for this behavior? 
                    </p>
                    <img data-src="img_contents/NNPDF-formal.png" style="width: 700px; height: auto;" class="plain"></img>
                    <div class="fragment" style="text-align:center; font-size: larger">  
                        <span>                
                        What if we perturb the PDF directly in the <b>PDF space </b>? 
                        </span>
                    </div>
                    <br>
                    <div class="fragment" style="text-align:left">                  
                        <span>Multidimensionality of the PDF space combined with the rotation, convolutions and the PDF-Observable correlations makes it difficult to perform systematic test and obtain a clear understanding of each flavor on the fit.</span>  
                    </div>
                </section>
                <!-- Slide 14.4 -->
                <section>
                    <h3 class="slide-title highlight" style="text-align: left;">NNPDF a black box model ?</h3>
                    <p style="text-align: left;">    
                        Black box system is a recurrent interpretability issue in modern DL. We can inspire from the method developed there to solve the interpretability problem of NNPDF. 
                    </p>
                    <img data-src="img_contents/blackbox_nnpdf.png" style="width: 700px; height: auto;" class="plain"></img>
                    <p style="text-align: center;">    
                       XAI answer : What's the impact of one feature \(\phi_j\) on the output of the model ?
                    </p>
                </section>
            </section>
            
            <!-- Slide 15: Shapley Values for NNPDF -->
            <section data-background-gradient="linear-gradient(to bottom, black, grey)">

                <!-- Slide 15.1 -->
                <section>
                    <h3 class="slide-title highlight" style="text-align: left;">Shapley Values</h3>
                    <p style="text-align: left;">
                        Shapley Values are inherited from game theory: represent the value of the <b>contribution \(\phi\)</b> of a player \(i\) within a coalition \(S\). 
                    </p>
                    \[
                    \phi_i = \sum_{S \subseteq N \setminus \{i\}}  \frac{|S|! \, (|N| - |S| - 1)!}{|N|!} \left[ v(S \cup \{i\}) - v(S) \right]
                    \]
                    <p style="text-align: left;">
                        where \(N\) is the set of all players, \(S\) is a subset of players not including \(i\), and \(v(S)\) is the value of the coalition \(S\). The main problem being the computational cost as it scales exponentially with the number of players \(2^N\). 
                    </p>
                    <p style="text-align: left;">
                        Largely popularized for its application to Machine Learning and Deep Learning, with methods like SHAP (SHapley Additive exPlanations) that makes them computable for high amount of players (features) given certain assumptions and approximations.
                    </p>
                    <div style="text-align:center">
                                <a href="https://arxiv.org/abs/1705.07874" style="text-align:center">
                                <img src="https://img.shields.io/badge/cs.AI-arXiv%3A1705.07874-B31B1B?logo=arxiv" height="25">                                </a>
                    </div>
                </section>

                <!-- Slide 15.2 -->
                <section>
                    <h3 class="slide-title highlight" style="text-align: left;">Shapley Value, the perfect closure test metric ?</h3>
                    <p style="text-align: left;">    
                        Applying Shapley Values to NNPDF, we treat each flavor as the feature of our black box model.  
                    </p>
                    <img data-src="img_contents/NNPDF-shapley.png" style="width: 700px; height: auto;" class="plain"></img>
                    <ul>
                        <li>\(\phi_i\) is the Shapley Value for flavor \(i\)</li>
                        <li>\(N\) is the total number of flavors.</li>
                        <li>\(S\) is a subset of flavors not including \(i\).</li>
                        <li>\(v(S)=\chi^2\) is the value of the coalition when all flavors in coalition \(S\) are perturbed, \(i\) and the rest of the flavors remain untouched.</li>
                    </ul>
                </section>


                <!-- Slide 15.3 -->
                <section>
                    <h3 class="slide-title highlight" style="text-align: left;">How to perturb the PDF space ?</h3>
                    <div class="container">
                        <div class="col" style="text-align: left;">
                        <span>
                        How do we perturb a PDF ? 
                        A smooth gaussian bump :
                        </span>
                        \[
                            f_j^{\text{pert}}(x)=f_j(x) + \delta_j(x)
                        \]

                        </div>
                        <div class="col">
                            <img data-src="img_contents/perturbed_pdf.png" style="width: 300px; height: auto;" class="plain"></img>
                        </div>
                    </div>

                    <div class="fragment" style="text-align:left">                  
                        <span>Choices remains to be done: </span>
                        <br>
                        <ul>
                            <li>How to choose the width and amplitude of the bump ?</li>
                            <li>How to choose the location of the bump ?</li>
                            <li>How to respect the physical constraints of the PDF ?</li> 
                            <li>Do we want similar perturbations for all flavors, or proportional to their contribution ?</li>
                        </ul>
                    </div>
                </section>

                <!-- Slide 15.4 -->
                <section>
                    <h3 class="slide-title highlight" style="text-align: left;">Coalitions and complexity</h3>

                    <p><strong>What we compute:</strong> the <em>exact</em> Shapley value for each PDF flavor by averaging its marginal contribution across all coalitions (all subsets of flavors).</p>
                    <p><strong>Computational cost:</strong> Exponential in the number of flavors, as we need to evaluate the fit for each coalition. In the flavor basis, \(N=9\) flavors, we have \(2^9 = 512\) coalitions.</p>
                    <p  style="text-align:left">Using exact Shapley values, we make <strong>no assumption of independence of the features.</strong></p>
                </section>

                <!-- Slide 15.5 -->
                <section>
                    <h3 class="slide-title highlight" style="text-align: left;">How to interpret the SV?</h3>
                    <div class="container">
                        <div class="col" style="text-align: left;">
                        <li>\(\phi_j< 0 \) flavor \(j\) is poorly constrained for this dataset.</li>  
                        <li>\(\phi_j> 0 \) flavor \(j\) is well constrained.</li>
                        <br>
                        <strong>\(x\) region dependency:</strong>
                        <span>
                        Perturbation is local and we can expect different contributions for different \(x\) regions.
                        </span>
                        <br>    
                        <br>
                        <strong>Dataset dependency:</strong>
                        <span>
                            \(\chi^2\) which is computed based on experimental dataset. We expect different contributions for different datasets.
                        </span>
                        </div>
                        <div class="col" style="align-items: center;">
                            <img data-src="img_contents/SV.png" style="width: 500px; height: auto;" class="plain"></img>
                            Exact Shapley Value have a completness property: 
                            \[\sum_{j \in N} \phi_j = v(N)-v(\emptyset)\]
                            and \(v(s)=\chi^2\) so they share the same unit. 
                        </div>
                    </div>
                </section>
            </section>

            <!-- Slide 16: Next steps -->
            <section data-background-image="img_contents/LHC.jpg">
                <h3 class="slide-title highlight" style="text-align: left;">Next steps:</h3>
                <div style="text-align: left;">   
                    <ul>             
                        <li>Implementation for proton proton collisions (for now only implemented for DIS). </li>
                        <li>Implementation to NNPDF.</li><br>
                        <li>Perturbation respectful of physical constraints.</li>
                        <li>Systematic study of the SV behavior for different perturbations and datasets.</li>
                    </ul>
                </div>
            </section>
         






















            <!-- PART C: Mechanistic Interpretability for Neutrino foundations model-->
 


            <section data-background-image="img_contents/IceCube_neutrino.jpg">
            <div style="background: rgba(0, 0, 0, 0.45); padding: 1.7rem 2.6rem; border-radius: 12px; max-width: 68%; margin: auto; backdrop-filter: blur(2px);">
                <h2 style="color: #ffffff; font-size: 2.1em; margin-bottom: 0.45em;">Mechanistic Interpretability for Neutrino foundations model</h2>
                <span style="font-size: smaller;"><b>Raphaël Bonnet-Guerrini</b><sup>1</sup>, Johann Ioannou-Nikolaides<sup>2</sup>, Troels Petersen<sup>2</sup>, Vincenzo Piuri <sup>1</sup>, Jean-Loup Tastet<sup>2</sup> and Inar Timiryasov<sup>2</sup></span>
                <br>
                <br>
                <span style="font-size: small; text-align:center"><sup>1</sup>Computer Science Department, University of Milan; &nbsp;&nbsp; <sup>2</sup>Physics Department, Københavns Universitet, Denmark</span>
            </div>
            </section>

            <section data-background-image="img_contents/IceCube_neutrino.jpg">
            <section>
                <div style="background: rgba(0, 0, 0, 0.45); padding: 1.7rem 2.6rem; border-radius: 12px; max-width: 90%; margin: auto; backdrop-filter: blur(2px);">
                <h3 class="slide-title highlight" style="text-align: left; color: #e0e0e0; font-weight: 300; font-size: 1em; line-height: 1.3; margin-bottom: 1em;">Foundation models</h3>
                <p style="text-align: left;">
                    Foundation models are large-scale machine learning models that are trained on a wide variety of data and can be adapted to a wide range of downstream tasks. They have shown remarkable performance in various domains, including natural language processing, computer vision, and more recently, in scientific domains such as physics.
                </p>
                <p style="text-align: left;"> Most foundation models rely on two specificities:</p>
                <ul>
                    <li>The self-attention mechanism of the transformer architecture.</li>
                    <li>The available volume of data and the possibility to perform Self-Supervised Learning.</li>
                </ul>
                <p>
                    <a href="https://arxiv.org/abs/2108.07258" style="text-align:center">
                        <img src="https://img.shields.io/badge/cs.ML-arXiv%3A2108.07258-B31B1B?logo=arxiv" height="25">                                
                    </a>
                    <a href="https://arxiv.org/abs/1706.03762" style="text-align:center">
                        <img src="https://img.shields.io/badge/cs.CL-arXiv%3A1706.03762-B31B1B?logo=arxiv" height="25">                                
                    </a>
                    <a href="https://arxiv.org/abs/1810.04805" style="text-align:center">
                        <img src="https://img.shields.io/badge/cs.CL-arXiv%3A1810.04805-B31B1B?logo=arxiv" height="25">                                
                    </a>
                </p>
                </div>
            </section>

            <section>
                <div style="background: rgba(0, 0, 0, 0.45); padding: 1.7rem 2.6rem; border-radius: 12px; max-width: 90%; margin: auto; backdrop-filter: blur(2px); display: block;">
                <h3 class="slide-title highlight" style="text-align: left; color: #e0e0e0; font-weight: 300; font-size: 1em; line-height: 1.3; margin-bottom: 1em;">Foundation model in Neutrino Physics</h3>
                <div class="container" style="display: flex; gap: 1rem; flex-wrap: wrap;">
                    <div class="col" style="flex: 1 1 45%; min-width: 240px; text-align: left;">
                    <p style="text-align: left;">
                        Neutrino detectors record pulses. <br> A pulse is a time series of the light emitted by the interaction of a neutrino with the detector. <br>It tracks the directions and energies of the incoming neutrinos.                         
                    </p>
                    <p style="text-align: left;">
                        <b>PolarBERT</b> is a foundation model trained on 131,000,000 high-quality Monte Carlo neutrino events.
                    </p>
                    <p style="text-align: left;">
                        Downstream task includes classification or reconstruction of the energy and direction.
                    </p>
                    </div>

                    <div class="col" style="flex: 1 1 45%; min-width: 240px;">
                    <img data-src="img_contents/neutrino_pulse.png" style="width: 400px; height: auto;" class="plain"></img>
                    <a href="https://ml4physicalsciences.github.io/2024/files/NeurIPS_ML4PS_2024_259.pdf">
                        <img src="https://img.shields.io/badge/%F0%9F%94%97-POLARBERT-0033A0" height="25">
                    </a>
                    </div>
                </div>
                </div>
            </section>    
            </section>    

            <section data-background-image="img_contents/IceCube_neutrino.jpg">
            <section>
                <div style="background: rgba(0, 0, 0, 0.45); padding: 1.7rem 2.6rem; border-radius: 12px; max-width: 90%; margin: auto; backdrop-filter: blur(2px);">
                <h3 class="slide-title highlight" style="text-align: left; color: #e0e0e0; font-weight: 300; font-size: 1em; line-height: 1.3; margin-bottom: 1em;">Mechanistic Interpretability</h3>
                <p style="text-align: left;">
                    Foundation Model are able to generalize.
                </p>
                <p style="text-align: center; font-size: larger;">
                    Mechanistic Interpretability ask: &#8680; How did my model solve this general class of problems? 
                </p>
                <div class="fragment">
                    <p style="text-align: left;">
                    One of the methodological is to reverse engineer the model identifying what"s the function of of each part
                    </p>
                    <p style="text-align: left;">
                    Natural decomposition (Neurons, Layers, Attention heads) fails because of polysemanticity caused by superposition (#feature>#neurons). 
                    </p>

                    <a href="https://arxiv.org/abs/2501.16496" style="text-align:center">
                    <img src="https://img.shields.io/badge/cs.LG-arXiv%3A2501.16496-B31B1B?logo=arxiv" height="25">                                
                    </a>

                    <a href="https://arxiv.org/abs/2209.10652" style="text-align:center">
                    <img src="https://img.shields.io/badge/cs.LG-arXiv%3A2209.10652-B31B1B?logo=arxiv" height="25">                                
                    </a>
                </div>
                </div>
            </section>

            <section>
                <div style="background: rgba(0, 0, 0, 0.45); padding: 1.7rem 2.6rem; border-radius: 12px; max-width: 90%; margin: auto; backdrop-filter: blur(2px);">
                <h3 class="slide-title highlight" style="text-align: left; color: #e0e0e0; font-weight: 300; font-size: 1em; line-height: 1.3; margin-bottom: 1em;">Sparse representations and Latent Spaces</h3>
                <p style="text-align: left;">
                    Polysemanticity is caused by overlapping directions in the activation space. Dimensionality reduction technique may not be able to disentangle the features.
                </p>
                <p style="text-align: left;">
                    To overcome this we can use Sparse Dictionary Learning (SDL) to find a sparse representation of the latent space.
                </p>
                <img data-src="img_contents/SDL.png" style="width: 500px; height: auto;" class="plain"></img>
                <p style="text-align: left;">
                    This allows the identification of behavior in the sparse latent space, providing insight on the network comprehension of underlying physics.
                </p>
                <a href="https://arxiv.org/abs/2510.23749" style="text-align:center">
                    <img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2510.23749-B31B1B?logo=arxiv" height="25">
                </a>
                </div>
            </section>
            </section>

            <section data-background-image="img_contents/IceCube_neutrino.jpg">
            <div style="background: rgba(0, 0, 0, 0.45); padding: 1.7rem 2.6rem; border-radius: 12px; max-width: 80%; margin: auto; backdrop-filter: blur(2px);">
                <h3 class="slide-title highlight" style="text-align: left; color: #e0e0e0; font-weight: 300; font-size: 1em; line-height: 1.3; margin-bottom: 1em;">Potential behavior to investigate:</h3>
                <ul>
                <li>Are there specific region for very long vs very short inputs?</li>
                <li>Muon vs Neutrino tracks.</li>
                <li>Does the model learn to identify the different light patterns of different neutrino flavors ?</li>
                </ul>
                <p>Any of your ideas!</p>
            </div>
            </section>














            <section>
                <section data-background-image="img_contents/gen_intro_img.png" >
                    <div class="title-panel">

                        <h3>Backup Slides</h3>
                    </div>
                </section>
                <section data-background-gradient="linear-gradient(to bottom, black, darkgrey)">
                    <h3 class="slide-title">Asymmetrical Co-Teaching</h3>
                    
                    <img data-src="img_contents/asym_coteaching.png" style="width: 800px; height: auto;" class="plain"></img>
                </section>
                <section data-background-image="img_contents/vera_rubin_obs_2.png">
                    <h3>Calibration Metrics</h3>
                        <ul>
                            <li>Negative Log-Likelihood (NLL): quality of probabilistic predictions.</li>
                            <li>Brier Score: MSE of probabilistic predictions.</li>
                            <li>Expected Calibration Error (ECE): difference between predicted probabilities and observed frequencies.</li>
                        </ul>
                        \[
                        \text{NLL} = -\frac{1}{N}\sum_{n=1}^{N}
                        \big[y_n \log \bar{p}_n + (1-y_n)\log(1-\bar{p}_n)\big]
                        \]
                        \[
                        \text{BS} = \frac{1}{N}\sum_{n=1}^{N}(p_n - y_n)^2
                        \]
                        \[
                        \text{ECE}=\sum_{m=1}^{M}\frac{|B_m|}{N}
                        \left|\text{acc}(B_m)-\text{conf}(B_m)\right|
                        \]


                </section>
            

                <section data-background-image="img_contents/LHC.jpg">
                    <h3 class="slide-title highlight" style="text-align: left;">Shapley Values Pseudo code</h3>

                    <div style="margin-top:0.6rem; padding:0.6rem; background:#0f1724; color:#e6eef6; border-radius:6px; font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, "Courier New", monospace; font-size:0.85rem; line-height:1.25;">
                    <pre style="margin:0;">
# INPUT: observables, mu,sigma,amplitude, n_samples, n_flavors = n
# OUTPUT: shapley_vals[], baseline_chi2, cache

baseline_chi2 = evaluate_chi2(observables, flavor_subset=[])   # v({})

cache = {}   # map subset -> v(S)
all_subsets = power_set(0..n-1)     # exclude full-set if desired

for i in 0..n-1:
SV = 0
for S in all_subsets:
    if i in S: continue

    # v(S)
    if S not in cache:
    cache[S] = evaluate_chi2(observables, flavor_subset=list(S), mu,sigma,amplitude, n_samples)
    vS = cache[S]

    # v(S ∪ {i})
    S_with = S ∪ {i}
    if S_with not in cache:
    cache[S_with] = evaluate_chi2(observables, flavor_subset=list(S_with), mu,sigma,amplitude, n_samples)
    vSw = cache[S_with]

    Δ = vSw - vS                                   # marginal contribution
    s = |S|
    w = factorial(s) * factorial(n - s - 1) / factorial(n)
    SV += w * Δ

shapley_vals[i] = SV

# RETURN: shapley_vals, baseline_chi2, evaluated_coalitions=|cache|
# COMPLEXITY: time ~ O(n · 2^n · cost_eval), space ~ O(2^n) (memoized)
                    </pre>
                    </div>
                </section>
                <section data-background-image="img_contents/LHC.jpg">
                    <h3 class="slide-title highlight" style="text-align: left;">Shapley Values weight</h3>
                    <span>
                        In game theory, Shapley Values represent the contribution \(\phi_i\) of a player \(i\) within a coalition \(S\) by comparing the outcomes of scenarios where the player is present  \(v(S \cup \{i\})\) versus absent \(v(S)\).
                    </span>
                    
                        \[
                        \phi_i = \sum_{S \subseteq N \setminus \{i\}} \text{w}(S) \left[ v(S \cup \{i\}) - v(S) \right]
                        \label{eq:comb_SV}
                        \]
                    <span>
                        With:
                    </span>
                        \[
                        w(S) = \frac{|S|! \, (|N| - |S| - 1)!}{|N|!}    
                        \] 
                    <span>
                        \(w(S)\)  weights for the importance of the coalition \(S\) being tested. It is the probability that the set of players who come after \(i\) is exactly \(S\).
                    </span>
                        <ul>
                        <li>\(|S|!\) is the number of ways to order the predecessors of \(i\)</li>
                        <li>\((|N| - |S| - 1)!\) is the number of way to order the players after \(i\)</li>
                        <li>\(|N|!\) is the number of way to order all players</li>
                        </ul>
                </section>
                <section data-background-image="img_contents/LHC.jpg">
                    <h3 class="slide-title highlight" style="text-align: left;">Shapley Values weight</h3>
                    <span>
                        This result is derived in the following way: 
                        Given \( |N| = n \), for a fixed player \(i\), let \( S \subseteq N \setminus \{i\} \) with \( |S| = s \).
                        The probability that player before \(i\) are exactly \(S\) can be decomposed in two conditions: 

                        First, \(i\) can be in any position in \(N\), so: 
                    </span>
                        \[
                        p(\text{pos}_i = s+1) = \frac{1}{n}
                        \]
                    <span>
                        Second, given that \( i \) is in position \( s+1 \), the \( s \) players before \( i \) form a uniformly random subset of the remaining \( n-1 \) players of size \(\binom{n-1}{s}\):
                    </span>
                        \[
                        p(\text{predecessors} = S \mid \text{pos}_i = s+1) = \frac{1}{\binom{n-1}{s}}
                        \]

                        <span>To fulfill these conditions, we multiply those probabilities:</span>
                        \[
                        \frac{1}{n}\cdot \frac{1}{\binom{n-1}{s}} = \frac{1}{n}\cdot \frac{s!(n-1-s)!}{(n-1)!} = w(S)
                        \] 
                </section>
                 <section data-background-image="img_contents/LHC.jpg">

                 </section>

            </section>






















            <section  data-background-image="img_contents/gen_intro_img.png">
                <div class="title-panel">
                    
                    <h1>Thank you ! </h1>
                
                <p class="meta">Contact : Raphaël.bonnet-guerrini@unimi.it</p>
                <p class="meta" style="font-size: small; text-align:center">
                    This work has received funding from the European Union's Horizon 2020 research and innovation programme under a Marie Skłodowska-Curie grant agreement.
                </p></div>
            </section>
        </div>
    </div>
</body>
<!-- Include Reveal.js -->
<script src="../assets/reveal/dist/reveal.js"></script>

<!-- Include Plugins (optional) -->
<script src="../assets/reveal/plugin/zoom/zoom.js"></script>
<script src="../assets/reveal/plugin/highlight/highlight.js"></script>

<!-- Include MathJax -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- Include Highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>

<script>
    // Initialize Reveal.js with hash and slide number options
    Reveal.initialize({
        hash: true,      // Enables URL hash navigation
        slideNumber: true, // Display slide numbers
        math: {
            mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        },
        plugins: [RevealZoom, RevealHighlight], // Initialize plugins
    });

    // Initialize Highlight.js for code blocks
    document.addEventListener('DOMContentLoaded', (event) => {
        document.querySelectorAll('pre code').forEach((block) => {
            hljs.highlightBlock(block);
        });
    });
</script>

</body>

</html>